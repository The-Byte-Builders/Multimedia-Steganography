{"cells":[{"cell_type":"markdown","source":["<h1>To mount drive</h1>"],"metadata":{"id":"tvJpFhJDmvuB"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3970,"status":"ok","timestamp":1676792991502,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"},"user_tz":-345},"id":"ZRG0HrYFnBOz","outputId":"efc09c6f-3633-4291-b58d-387e19bcb486"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# to change the working directory\n","%cd /content/drive/MyDrive/Colab Notebooks/Steganography"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1p2iSgy6pdpP","executionInfo":{"status":"ok","timestamp":1676792994481,"user_tz":-345,"elapsed":457,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"03462587-a7e8-4aed-94d9-18f6fd3dc238"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1oaZCwXh2SHDAQ3FtWzvS1pKkn3EYFRo1/Colab Notebooks/Steganography\n"]}]},{"cell_type":"markdown","source":["<h1>The Imports</h1>"],"metadata":{"id":"6cBTM_bKm2iG"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"3or6R6we1LZV","executionInfo":{"status":"ok","timestamp":1676793006031,"user_tz":-345,"elapsed":4882,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np \n","import pandas as pd\n","import torchvision\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import joblib\n","import tempfile\n","import cv2\n","import subprocess\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","tempfile.tempdir = \"/tmp\""]},{"cell_type":"markdown","source":["<h1>To Extract Audio From Video</h1>"],"metadata":{"id":"k9TLn3lBuLxd"}},{"cell_type":"code","source":["pip install ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oacZXuj7jcWZ","executionInfo":{"status":"ok","timestamp":1676793017997,"user_tz":-345,"elapsed":6845,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"11a9ebbd-f766-42ae-c2f5-c7aff83dfcb1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ffmpeg\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: ffmpeg\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=66208b2d9b6d1e989576635238e740a7097925da7d5978b8758d3cc40a57fc48\n","  Stored in directory: /root/.cache/pip/wheels/30/33/46/5ab7eca55b9490dddbf3441c68a29535996270ef1ce8b9b6d7\n","Successfully built ffmpeg\n","Installing collected packages: ffmpeg\n","Successfully installed ffmpeg-1.4\n"]}]},{"cell_type":"code","source":["# To extract audio from video \n","\n","def convert_video_to_audio(video_file, output_ext=\"wav\"):\n","    filename, ext = os.path.splitext(video_file)\n","    subprocess.call([\"ffmpeg\", \"-y\", \"-i\", video_file, f\"{filename}.{output_ext}\"], \n","                    stdout=subprocess.DEVNULL,\n","                    stderr=subprocess.STDOUT)\n","\n","\n","\n","\n"],"metadata":{"id":"tN6arm7SfcBI","executionInfo":{"status":"ok","timestamp":1676793041828,"user_tz":-345,"elapsed":467,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["vf = 'Video and Audio files/covervid.mp4'\n","convert_video_to_audio(vf)"],"metadata":{"id":"js8LkwClf3Go","executionInfo":{"status":"ok","timestamp":1676793045578,"user_tz":-345,"elapsed":1557,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["<h1>To make temporary file of list of frames"],"metadata":{"id":"_kBMUyQXmTQP"}},{"cell_type":"code","source":["# Creating temporary file and deleting it (experiment)\n","\n","temp = tempfile.TemporaryFile()\n","\n","try:\n","    a = np.array([[1, 2, 3],[4, 5, 6]])\n","    b = np.array([[45,67 , 63],[14, 25, 36]])\n","    print(\"Original array:\")\n","    print(a)\n","    print(a.shape)\n","    a_bytes = a.tobytes()\n","    b_bytes = b.tobytes()\n","    print(len(a_bytes))\n","    print(len(b_bytes))\n","    temp.write(a_bytes)\n","    temp.write(b_bytes)\n","    temp.seek(0)\n","    \n","    \n","    print(\"After loading, content of the temp file:\")\n","    for i in range(2):\n","      read_bytes = temp.read(len(a_bytes))\n","      print(\"in the temp file: \", read_bytes)\n","      a2 = np.frombuffer(read_bytes, dtype=a.dtype)\n","      a2 = a2.reshape(a.shape)\n","      print(a2)\n","    #print(temp.read())\n","   \n","finally:\n","    temp.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUjRkmnXovaV","executionInfo":{"status":"ok","timestamp":1676793096534,"user_tz":-345,"elapsed":460,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"d3250b5f-8609-4030-a008-7e55c580a39e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Original array:\n","[[1 2 3]\n"," [4 5 6]]\n","(2, 3)\n","48\n","48\n","After loading, content of the temp file:\n","in the temp file:  b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n","[[1 2 3]\n"," [4 5 6]]\n","in the temp file:  b'-\\x00\\x00\\x00\\x00\\x00\\x00\\x00C\\x00\\x00\\x00\\x00\\x00\\x00\\x00?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x00\\x00\\x00\\x00\\x00\\x00\\x00$\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n","[[45 67 63]\n"," [14 25 36]]\n"]}]},{"cell_type":"code","source":["cover_vid = 'Video and Audio files/covervid.mp4'\n","secret_vid = 'Video and Audio files/secretvid.mp4'"],"metadata":{"id":"uq6AxbjxPW69","executionInfo":{"status":"ok","timestamp":1676793755995,"user_tz":-345,"elapsed":461,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# cover video to frames in a temporary file:\n","cap1 = cv2.VideoCapture(cover_vid)\n","fps1 = cap1.get(cv2.CAP_PROP_FPS)\n","temp_cover = tempfile.TemporaryFile()\n","count1 = 0\n","while True:\n","      is_read, frame = cap1.read()\n","      if not is_read:\n","          # break out of the loop if there are no frames to read\n","          break\n","      if count1 == 0:\n","        cover_frame = frame\n","      temp_cover.write(frame.tobytes())\n","      count1 += 1\n","temp_cover.seek(0)\n","print('total number of frames in cover =', count1)\n","\n","\n","# secret video to frames in a temporary file:\n","\n","cap2 = cv2.VideoCapture(secret_vid)\n","fps2 = cap2.get(cv2.CAP_PROP_FPS)\n","temp_secret = tempfile.TemporaryFile()\n","count2 = 0\n","while True:\n","      is_read, frame = cap2.read()\n","      if not is_read:\n","          # break out of the loop if there are no frames to read\n","          break\n","      if count2 == 0:\n","        secret_frame = frame\n","      temp_secret.write(frame.tobytes())\n","      count2 += 1\n","temp_secret.seek(0)\n","print('total number of frames in secret =', count2)\n"],"metadata":{"id":"77DoArFaAwZY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676795379477,"user_tz":-345,"elapsed":51851,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"6d47c000-bb6b-471a-b44b-5179f8c577d6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["total number of frames in cover = 2403\n","total number of frames in secret = 893\n"]}]},{"cell_type":"code","source":["# checking if the above code did the right job\n","\n","cover_frame_bytes = cover_frame.tobytes()\n","secret_frame_bytes = secret_frame.tobytes()\n","read_cover_bytes = temp_cover.read(len(cover_frame_bytes))\n","frame_obt = np.frombuffer(read_cover_bytes, dtype = cover_frame.dtype)\n","frame_obt = frame_obt.reshape(cover_frame.shape)\n","print(\"obtained frame :\" , frame_obt[:5])\n","print(\"the original and the obtained are same : \" , np.array_equal(frame_obt,cover_frame))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEjmJcOSYTHw","executionInfo":{"status":"ok","timestamp":1676795384514,"user_tz":-345,"elapsed":474,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"56e07c97-d16d-4b82-cea7-6b1c60e2f112"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["obtained frame : [[[238 214 156]\n","  [238 214 156]\n","  [238 214 156]\n","  ...\n","  [232 211 194]\n","  [231 210 193]\n","  [231 210 193]]\n","\n"," [[238 214 156]\n","  [238 214 156]\n","  [238 214 156]\n","  ...\n","  [231 210 193]\n","  [231 210 193]\n","  [231 210 193]]\n","\n"," [[238 214 156]\n","  [238 214 156]\n","  [238 214 156]\n","  ...\n","  [231 210 193]\n","  [231 210 193]\n","  [231 210 193]]\n","\n"," [[238 214 156]\n","  [238 214 156]\n","  [238 214 156]\n","  ...\n","  [231 210 193]\n","  [231 210 193]\n","  [231 210 193]]\n","\n"," [[238 214 156]\n","  [238 214 156]\n","  [238 214 156]\n","  ...\n","  [231 210 193]\n","  [231 210 193]\n","  [231 210 193]]]\n","the original and the obtained are same :  True\n"]}]},{"cell_type":"code","source":["# close the temp file, which will also delete it\n","\n","temp_cover.close()\n","temp_secret.close()"],"metadata":{"id":"G45IbnI0ZKg6","executionInfo":{"status":"ok","timestamp":1676795542287,"user_tz":-345,"elapsed":1215,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["<h1>To push this code to Github</h1>"],"metadata":{"id":"bo4N0B3s0ff1"}},{"cell_type":"code","source":["!git config --global user.email \"manohardahal40@gmail.com\"\n","!git config --global user.name \"manohar322028\"\n","\n","%cd /content/drive/MyDrive\n","!git init 'Colab Notebooks'\n","\n","%cd /content/drive/MyDrive/Colab Notebooks/Steganography\n","\n","!git add NOTEBOOKS/Video_Steganography.ipynb\n","\n","!git commit -m \"creation of temporary files\"\n","\n","!git push origin master\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhQoiZZ90lYm","executionInfo":{"status":"ok","timestamp":1676797322372,"user_tz":-345,"elapsed":1933,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"17c634bc-d7fc-48ea-be49-0c3a00112105"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","Reinitialized existing Git repository in /content/drive/.shortcut-targets-by-id/1oaZCwXh2SHDAQ3FtWzvS1pKkn3EYFRo1/Colab Notebooks/.git/\n","/content/drive/.shortcut-targets-by-id/1oaZCwXh2SHDAQ3FtWzvS1pKkn3EYFRo1/Colab Notebooks/Steganography\n","Auto packing the repository in background for optimum performance.\n","See \"git help gc\" for manual housekeeping.\n","[master 60b8eda] creation of temporary files\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite Steganography/NOTEBOOKS/Video_Steganography.ipynb (83%)\n","fatal: could not read Username for 'https://github.com': No such device or address\n"]}]},{"cell_type":"markdown","source":["<h1>For Model</h1>"],"metadata":{"id":"F65UhLW6nwAZ"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"H2R_S-By1s-L","executionInfo":{"status":"ok","timestamp":1676786386026,"user_tz":-345,"elapsed":547,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}}},"outputs":[],"source":["root_dir = '/content/drive/MyDrive/Colab Notebooks/'\n","dataset_path = os.path.join(root_dir,'Steganography/Dataset')\n","\n","train_csv = os.path.join(root_dir,'train_dataset.csv')\n","valid_csv = os.path.join(root_dir,'validation_dataset.csv')\n","test_csv = os.path.join(dataset_path,'validation_dataset.csv')\n","train_folder = os.path.join(dataset_path,'train')\n","validation_folder = os.path.join(dataset_path,'valid')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXAM3lgbcaOf"},"outputs":[],"source":["cover = '/content/drive/MyDrive/Colab Notebooks/Steganography/Lion.jpg'\n","secret = '/content/drive/MyDrive/Colab Notebooks/Steganography/Tiger.jpg'\n","\n","dataset = [{\n","        'cover_image':cover,\n","        'secret_image':secret\n","    }]\n","dataframe = pd.DataFrame(dataset)\n","cover = Image.open(cover)\n","secret = Image.open(secret)\n"]},{"cell_type":"code","source":["video_file2 = os.path.join(root_dir,'video_file.mp4')"],"metadata":{"id":"uVH53zLyV71d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# framelist2, framecount2 = toFrames(video_file2)\n","toFrames(video_file2)"],"metadata":{"id":"BhSp11paOy7J","executionInfo":{"status":"ok","timestamp":1676000566192,"user_tz":-345,"elapsed":27442,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f85699d-8c9e-475d-fda5-d6069b960253"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total number of frames = 893\n"]}]},{"cell_type":"code","source":["cap1 = cv2.VideoCapture(video_file1)\n","fps1 = cap1.get(cv2.CAP_PROP_FPS)\n","\n","cap2 = cv2.VideoCapture(video_file2)\n","fps2 = cap2.get(cv2.CAP_PROP_FPS)\n","\n","del cap1\n","del cap2"],"metadata":{"id":"l4W8Gtw0WZ4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# frames = [os.path.join(root_dir,'video_file-opencv',item) for item in dirs]\n","# frames[:5]"],"metadata":{"id":"yxTQf6YD-Vvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dirs1=os.listdir(os.path.join(root_dir,'covervid-opencv'))\n","print(len(dirs1))"],"metadata":{"id":"rZHVGyF2ojbu","executionInfo":{"status":"ok","timestamp":1676000787620,"user_tz":-345,"elapsed":869,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7dd70efd-9618-4f64-8277-969321b16946"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2402\n"]}]},{"cell_type":"code","source":["frames1 = [os.path.join(root_dir,'video_file-opencv',item) for item in dirs1]"],"metadata":{"id":"fm8iqbpRtJqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dirs2=os.listdir(os.path.join(root_dir,'video_file-opencv'))\n","print(len(dirs2))"],"metadata":{"id":"XPCydbJjtcnU","executionInfo":{"status":"ok","timestamp":1676000800086,"user_tz":-345,"elapsed":613,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b884c081-1f9b-41b6-f814-fdd12e42bad0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["893\n"]}]},{"cell_type":"code","source":["frames2 = [os.path.join(root_dir,'video_file-opencv',item) for item in dirs2]"],"metadata":{"id":"TCH38WSIt9Mb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(frames2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjcn3LRKjgL9","executionInfo":{"status":"ok","timestamp":1676003372397,"user_tz":-345,"elapsed":20,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"outputId":"fb0ddcb6-c6e0-4ccb-9612-760e81e66bea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["893"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["cover = Image.open(frames1[0])"],"metadata":{"id":"dJwnNExw2zKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cover.size"],"metadata":{"id":"1ePVGPLM3BdX","executionInfo":{"status":"ok","timestamp":1676000811005,"user_tz":-345,"elapsed":599,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab19cbb8-f163-452f-e578-72fc83159715"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(600, 600)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnMSTB4b1d6j"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9w98z0qOpipP"},"outputs":[],"source":["\n","IMG_SIZE = 64\n","LEARNING_RATE  = 0.001\n","COVER_LOSS_WEIGHT = 1\n","SECRET_LOSS_WEIGHT = 1\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 1\n","EPOCHS = 1000\n","DECODER_LOSS_WEIGHT = 1"]},{"cell_type":"code","source":["cover.size"],"metadata":{"id":"aa-CvM5u57Sx","executionInfo":{"status":"ok","timestamp":1676000847155,"user_tz":-345,"elapsed":19,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7de79445-3dec-44fc-bd7d-19c7419a8da7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(600, 600)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# to reverse\n","cover_size = tuple(reversed(cover.size))"],"metadata":{"id":"RGBswKjl5nRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5b5sJ8W4TUJ"},"outputs":[],"source":["imagetransformation = {\n","    'train_transforms':torchvision.transforms.Compose([torchvision.transforms.Resize((IMG_SIZE,IMG_SIZE)),torchvision.transforms.ToTensor()]),\n","    'valid_transforms':torchvision.transforms.Compose([torchvision.transforms.Resize((IMG_SIZE,IMG_SIZE)),torchvision.transforms.ToTensor()]),\n","    'test_transforms':torchvision.transforms.Compose([torchvision.transforms.Resize(cover.size),torchvision.transforms.ToTensor()])\n","}\n","#conversion of image into resized format and return of trnasformed image\n","class SteganoDataset(torch.utils.data.Dataset):\n","    def __init__(self,dataset_csv,transforms,type_of_dataset,size='complete'):\n","        #self.dataset = pd.read_csv(dataset_csv)\n","        self.dataset = dataset_csv\n","        self.dataset = self.dataset.reset_index(drop=True)\n","        if size !='complete':\n","            self.dataset = self.dataset[:2]\n","        self.transforms = transforms\n","        self.type = type_of_dataset\n","    \n","    def __getitem__(self,index):\n","        cover_image = self.dataset.iloc[index]['cover_image']\n","        secret_image = self.dataset.iloc[index]['secret_image']\n","        # print(cover_image)\n","        #cover_image = Image.open(os.path.join('/content/drive/MyDrive/Colab Notebooks','Valid',cover_image))\n","        #secret_image = Image.open(os.path.join('/content/drive/MyDrive/Colab Notebooks','Valid',secret_image))\n","        transformed_cover_image = self.transforms(cover)\n","        transformed_secret_image = self.transforms(secret)\n","\n","        if self.type == 'train':\n","            return {\n","                'cover_image':transformed_cover_image,\n","                'secret_image':transformed_secret_image,\n","            }\n","        else:\n","            return {\n","                'cover_image':transformed_cover_image,\n","                'secret_image':transformed_secret_image,\n","            }\n","    \n","    def __len__(self):\n","      return len(self.dataset)\n","class PrepNetwork1(nn.Module):\n","    def __init__(self):\n","        super(PrepNetwork1, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=3,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=3,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv4 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv6 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","    \n","    def forward(self,secret_image):\n","        output_1 = F.relu(self.conv1(secret_image))\n","        output_2 = F.relu(self.conv2(secret_image))\n","        output_3 = F.relu(self.conv3(secret_image))\n","        \n","        concatenated_image = torch.cat([output_1,output_2,output_3],dim=1)\n","        output_4 = F.relu(self.conv4(concatenated_image))\n","        output_5 = F.relu(self.conv5(concatenated_image))\n","        output_6 = F.relu(self.conv6(concatenated_image))\n","        \n","        final_concat_image = torch.cat([output_4,output_5,output_6],dim=1)\n","        return final_concat_image\n","\n","class HidingNetwork(nn.Module):\n","    def __init__(self):\n","        super(HidingNetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=68,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=68,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=68,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv4 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv6 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv7 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv8 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv9 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv10 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv11 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv12 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv13 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv14 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv15 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.final_layer = nn.Conv2d(in_channels=65,out_channels=3,kernel_size=(3,3),stride=1,padding=1)\n","        \n","    def forward(self,secret_image_1,cover_image):\n","        concatenated_secrets = torch.cat([cover_image,secret_image_1],dim=1)\n","        \n","        output_1 = F.relu(self.conv1(concatenated_secrets))\n","        output_2 = F.relu(self.conv2(concatenated_secrets))\n","        output_3 = F.relu(self.conv3(concatenated_secrets))\n","        concat_1 = torch.cat([output_1,output_2,output_3],dim=1)\n","        \n","        output_4 = F.relu(self.conv4(concat_1))\n","        output_5 = F.relu(self.conv5(concat_1))\n","        output_6 = F.relu(self.conv6(concat_1))\n","        concat_2 = torch.cat([output_4,output_5,output_6],dim=1)\n","        \n","        output_7 = F.relu(self.conv7(concat_2))\n","        output_8 = F.relu(self.conv8(concat_2))\n","        output_9 = F.relu(self.conv9(concat_2))\n","        concat_3 = torch.cat([output_7,output_8,output_9],dim=1)\n","        \n","        output_10 = F.relu(self.conv10(concat_3))\n","        output_11 = F.relu(self.conv11(concat_3))\n","        output_12 = F.relu(self.conv12(concat_3))\n","        concat_4 = torch.cat([output_10,output_11,output_12],dim=1)\n","        \n","        output_13 = F.relu(self.conv13(concat_4))\n","        output_14 = F.relu(self.conv14(concat_4))\n","        output_15 = F.relu(self.conv15(concat_4))\n","        concat_5 = torch.cat([output_13,output_14,output_15],dim=1)\n","        \n","        output_converted_image = F.relu(self.final_layer(concat_5))\n","        \n","        return output_converted_image\n","class Encoder(nn.Module):\n","    def __init__(self,prep_network_1,hiding_network):\n","        super(Encoder, self).__init__()\n","        self.prep_network1 = prep_network_1\n","        self.hiding_network = hiding_network\n","    \n","    def forward(self,cover_image,secret_image_1):\n","        encoded_secret_image_1 = self.prep_network1(secret_image_1)\n","        \n","        hidden_image = self.hiding_network(encoded_secret_image_1,\n","                                           cover_image\n","                                          )\n","#         hidden_image = (0.01**0.5)*torch.randn(hidden_image.size(),device=device)\n","        return hidden_image\n","class RevealNetwork1(nn.Module):\n","    def __init__(self):\n","        super(RevealNetwork1,self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=3,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=3,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv4 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv6 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv7 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv8 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv9 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv10 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv11 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv12 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.conv13 = nn.Conv2d(in_channels=65,out_channels=50,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv14 = nn.Conv2d(in_channels=65,out_channels=10,kernel_size=(3,3),stride=1,padding=1)\n","        self.conv15 = nn.Conv2d(in_channels=65,out_channels=5,kernel_size=(5,5),stride=1,padding=2)\n","        \n","        self.final_layer = nn.Conv2d(in_channels=65,out_channels=3,kernel_size=(3,3),stride=1,padding=1)    \n","    \n","    def forward(self,hidden_image):\n","        \n","        output_1 = F.relu(self.conv1(hidden_image))\n","        output_2 = F.relu(self.conv2(hidden_image))\n","        output_3 = F.relu(self.conv3(hidden_image))\n","        concat_1 = torch.cat([output_1,output_2,output_3],dim=1)\n","        \n","        output_4 = F.relu(self.conv4(concat_1))\n","        output_5 = F.relu(self.conv5(concat_1))\n","        output_6 = F.relu(self.conv6(concat_1))\n","        concat_2 = torch.cat([output_4,output_5,output_6],dim=1)\n","        \n","        output_7 = F.relu(self.conv7(concat_2))\n","        output_8 = F.relu(self.conv8(concat_2))\n","        output_9 = F.relu(self.conv9(concat_2))\n","        concat_3 = torch.cat([output_7,output_8,output_9],dim=1)\n","        \n","        output_10 = F.relu(self.conv10(concat_3))\n","        output_11 = F.relu(self.conv11(concat_3))\n","        output_12 = F.relu(self.conv12(concat_3))\n","        concat_4 = torch.cat([output_10,output_11,output_12],dim=1)\n","        \n","        output_13 = F.relu(self.conv13(concat_4))\n","        output_14 = F.relu(self.conv14(concat_4))\n","        output_15 = F.relu(self.conv15(concat_4))\n","        concat_5 = torch.cat([output_13,output_14,output_15],dim=1)\n","        \n","        output_revealed_image = F.relu(self.final_layer(concat_5))\n","        \n","        return output_revealed_image\n","class Decoder(nn.Module):\n","    def __init__(self,reveal_network_1):\n","        super(Decoder, self).__init__()\n","        self.reveal_network_1 = reveal_network_1\n","    \n","    def forward(self,hidden_image):\n","        reveal_image_1 = self.reveal_network_1(hidden_image)\n","        return reveal_image_1\n","class SteganoModel(nn.Module):\n","    def __init__(self,encoder,decoder):\n","        super(SteganoModel,self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","    \n","    def forward(self,cover_image,secret_image_1,hidden_image,mode):\n","        if mode == 'full':\n","            for param in self.encoder.parameters():\n","                param.requires_grad = True\n","            for param in self.decoder.parameters():\n","                param.requires_grad = False\n","            hidden_image = self.encoder(cover_image,secret_image_1)\n","            reveal_image_1 = self.decoder(hidden_image)\n","            return hidden_image,reveal_image_1\n","        elif mode == 'encoder':\n","            for param in self.encoder.parameters():\n","                param.requires_grad = False\n","            for param in self.decoder.parameters():\n","                param.requires_grad = False\n","            hidden_image = self.encoder(cover_image,secret_image_1)\n","            return hidden_image\n","        elif mode == 'decoder':\n","            for param in self.encoder.parameters():\n","                param.requires_grad = False\n","            for param in self.decoder.parameters():\n","                param.requires_grad = True\n","            \n","            reveal_image1 = self.decoder(hidden_image)\n","            return reveal_image1\n","\n","filename = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Steganography/Completed_model.joblib\")\n","loaded_model = joblib.load(filename)\n","#joblib.dump(model, filename)\n","prep_1 = PrepNetwork1()\n","hiding_network = HidingNetwork()\n","\n","encoder = Encoder(prep_1,hiding_network)\n","\n","reveal_1 = RevealNetwork1()\n","\n","\n","decoder = Decoder(reveal_1)\n","\n","model = SteganoModel(encoder,decoder)\n","model.to(device)\n","class SteganoLoss(nn.Module):\n","    def __init__(self,cover_weight,secret_weight):\n","        super(SteganoLoss,self).__init__()\n","        self.cover_weight = cover_weight\n","        self.secret_weight = secret_weight\n","        \n","    def forward(self,predicted_cover_image,cover_image,\n","               predicted_secret_image_1,secret_image_1):\n","    \n","        cover_loss = self.cover_weight*(F.mse_loss(predicted_cover_image,cover_image))\n","        secret_loss = self.secret_weight*(F.mse_loss(predicted_secret_image_1,secret_image_1)) \n","        return cover_loss + secret_loss\n","\n","class DecoderLoss(nn.Module):\n","    def __init__(self,decoder_loss_weight):\n","        super(DecoderLoss,self).__init__()\n","        self.decoder_loss_weight = decoder_loss_weight\n","    \n","    def forward(self,reveal_output1,secret_image_1):\n","        reveal1 = self.decoder_loss_weight*F.mse_loss(reveal_output1,secret_image_1)\n","        return reveal1\n","'''\n","#training_csv_path = os.path.join(dataset_path,train_csv)\n","#validation_csv_path = os.path.join(dataset_path,valid_csv)\n","test_csv_path = os.path.join('/content/drive/MyDrive/Colab Notebooks','test_dataset.csv')\n","\n","#training_dataset = SteganoDataset(training_csv_path,imagetransformation['train_transforms'],'train','complete')\n","#valid_dataset = SteganoDataset(validation_csv_path,imagetransformation['valid_transforms'],'valid')\n","test_dataset = SteganoDataset(test_csv_path,imagetransformation['test_transforms'],'Valid')\n","print(test_dataset)\n","'''\n","test_dataset = SteganoDataset(dataframe,imagetransformation['test_transforms'],'Valid')\n","\n","'''\n","train_data_loader = torch.utils.data.DataLoader(training_dataset, \n","                                                batch_size = TRAIN_BATCH_SIZE, \n","                                                shuffle=True,\n","                                               drop_last = True,\n","                                               num_workers = 0\n","                                               )\n","valid_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","                                                batch_size = VALID_BATCH_SIZE, \n","                                                shuffle=True,\n","                                                drop_last = True,\n","                                                num_workers = 0\n","                                               )\n","                                               '''\n","test_data_loader = torch.utils.data.DataLoader(test_dataset, \n","                                                batch_size = VALID_BATCH_SIZE, \n","                                                shuffle=True,\n","                                                drop_last = True,\n","                                                num_workers = 0\n","                                               )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hvQLeCfjyug","executionInfo":{"status":"ok","timestamp":1676000853318,"user_tz":-345,"elapsed":33,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"1a474b07-a031-42c4-cc40-486388f0868e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfull_model_criterion = SteganoLoss(SECRET_LOSS_WEIGHT,COVER_LOSS_WEIGHT)\\n\\nfull_model_optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\\n\\ndecoder_criterion = DecoderLoss(DECODER_LOSS_WEIGHT)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["'''\n","full_model_criterion = SteganoLoss(SECRET_LOSS_WEIGHT,COVER_LOSS_WEIGHT)\n","\n","full_model_optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n","\n","decoder_criterion = DecoderLoss(DECODER_LOSS_WEIGHT)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZVmw3xWUFum","executionInfo":{"status":"ok","timestamp":1676000853320,"user_tz":-345,"elapsed":31,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/","height":157},"outputId":"e652cda1-0de9-4f66-a523-c15f8e740269"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef train(model,epochs,decoder_criterion,full_model_optimizer,full_model_criterion,learning_rate,training_iterator,valid_iterator,print_every=50):\\n    \\n    training_full_model_loss_list = []\\n    decoder_loss_list = []\\n    valid_loss_list = []\\n    \\n    for epoch in range(epochs):\\n        for index,training_dict in enumerate(training_iterator):\\n\\n            cover_image = training_dict[\\'cover_image\\']\\n            cover_image = cover_image.to(device)\\n\\n            secret_image_1 = training_dict[\\'secret_image\\']\\n            secret_image_1 = secret_image_1.to(device)\\n\\n            \\n            full_model_optimizer.zero_grad()\\n\\n            encoder_output = model(cover_image,secret_image_1,secret_image_1,\\'encoder\\')\\n\\n            hidden_image,reveal_image_1 = model(cover_image,secret_image_1,secret_image_1,\\n                                                                \\'full\\')\\n\\n            full_model_loss = full_model_criterion(hidden_image,cover_image,\\n                             reveal_image_1,secret_image_1\\n                            )\\n            full_model_loss.backward()\\n            full_model_optimizer.step()\\n\\n            full_model_optimizer.zero_grad()\\n            reveal_output1 = model(cover_image,secret_image_1,\\n                                                                 encoder_output,\\'decoder\\')\\n            decoder_loss = decoder_criterion(reveal_output1,secret_image_1)\\n\\n            decoder_loss.backward()\\n            full_model_optimizer.step()\\n\\n        training_full_model_loss_list.append(full_model_loss)\\n        decoder_loss_list.append(decoder_loss)\\n        if epoch % print_every == 0:\\n            print(\"Training full model loss at {} epochs is: {}\".format(epoch, full_model_loss))\\n            print(\"Training decoder loss at {} epochs is: {}\".format(epoch, decoder_loss))\\n\\n    return model, training_full_model_loss_list,decoder_loss_list\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}],"source":["'''\n","def train(model,epochs,decoder_criterion,full_model_optimizer,full_model_criterion,learning_rate,training_iterator,valid_iterator,print_every=50):\n","    \n","    training_full_model_loss_list = []\n","    decoder_loss_list = []\n","    valid_loss_list = []\n","    \n","    for epoch in range(epochs):\n","        for index,training_dict in enumerate(training_iterator):\n","\n","            cover_image = training_dict['cover_image']\n","            cover_image = cover_image.to(device)\n","\n","            secret_image_1 = training_dict['secret_image']\n","            secret_image_1 = secret_image_1.to(device)\n","\n","            \n","            full_model_optimizer.zero_grad()\n","\n","            encoder_output = model(cover_image,secret_image_1,secret_image_1,'encoder')\n","\n","            hidden_image,reveal_image_1 = model(cover_image,secret_image_1,secret_image_1,\n","                                                                'full')\n","\n","            full_model_loss = full_model_criterion(hidden_image,cover_image,\n","                             reveal_image_1,secret_image_1\n","                            )\n","            full_model_loss.backward()\n","            full_model_optimizer.step()\n","\n","            full_model_optimizer.zero_grad()\n","            reveal_output1 = model(cover_image,secret_image_1,\n","                                                                 encoder_output,'decoder')\n","            decoder_loss = decoder_criterion(reveal_output1,secret_image_1)\n","\n","            decoder_loss.backward()\n","            full_model_optimizer.step()\n","\n","        training_full_model_loss_list.append(full_model_loss)\n","        decoder_loss_list.append(decoder_loss)\n","        if epoch % print_every == 0:\n","            print(\"Training full model loss at {} epochs is: {}\".format(epoch, full_model_loss))\n","            print(\"Training decoder loss at {} epochs is: {}\".format(epoch, decoder_loss))\n","\n","    return model, training_full_model_loss_list,decoder_loss_list\n","'''"]},{"cell_type":"markdown","metadata":{"id":"rE94MW8u5_2n"},"source":["Training full model loss at 0 epochs is: 0.14862200617790222\n","Training decoder loss at 0 epochs is: 0.056159209460020065\n","Training full model loss at 50 epochs is: 0.010316253639757633\n","Training decoder loss at 50 epochs is: 0.006905762478709221\n","Training full model loss at 100 epochs is: 0.005915718153119087\n","Training decoder loss at 100 epochs is: 0.004135390743613243\n","Training full model loss at 150 epochs is: 0.00597141869366169\n","Training decoder loss at 150 epochs is: 0.003256219904869795\n","Training full model loss at 200 epochs is: 0.004562869668006897\n","Training decoder loss at 200 epochs is: 0.0032390698324888945\n","Training full model loss at 250 epochs is: 0.004033954814076424\n","Training decoder loss at 250 epochs is: 0.00324784847907722\n","Training full model loss at 300 epochs is: 0.0028372027445584536\n","Training decoder loss at 300 epochs is: 0.002142705488950014\n","Training full model loss at 350 epochs is: 0.003511040238663554\n","Training decoder loss at 350 epochs is: 0.0028315880335867405\n","Training full model loss at 400 epochs is: 0.006889783777296543\n","Training decoder loss at 400 epochs is: 0.006818610243499279\n","Training full model loss at 450 epochs is: 0.0030810432508587837\n","Training decoder loss at 450 epochs is: 0.0027422530110925436\n","Training full model loss at 500 epochs is: 0.0026186055038124323\n","Training decoder loss at 500 epochs is: 0.00222468632273376\n","Training full model loss at 550 epochs is: 0.0022935508750379086\n","Training decoder loss at 550 epochs is: 0.0016986110713332891\n","Training full model loss at 600 epochs is: 0.0017199815483763814\n","Training decoder loss at 600 epochs is: 0.0012137810699641705\n","Training full model loss at 650 epochs is: 0.002981864381581545\n","Training decoder loss at 650 epochs is: 0.002139737131074071\n","Training full model loss at 700 epochs is: 0.00157183688133955\n","Training decoder loss at 700 epochs is: 0.0011825517285615206\n","Training full model loss at 750 epochs is: 0.001144192647188902\n","Training decoder loss at 750 epochs is: 0.0006921886233612895\n","Training full model loss at 800 epochs is: 0.0016071755671873689\n","Training decoder loss at 800 epochs is: 0.0011148437624797225\n","Training full model loss at 850 epochs is: 0.0016389719676226377\n","Training decoder loss at 850 epochs is: 0.0010738647542893887\n","Training full model loss at 900 epochs is: 0.0011910195462405682\n","Training decoder loss at 900 epochs is: 0.0007727841730229557\n","Training full model loss at 950 epochs is: 0.0011191064259037375\n","Training decoder loss at 950 epochs is: 0.0007212755153886974"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Zo_lYACU0o9"},"outputs":[],"source":["#model, training_full_model_loss_list,decoder_loss_list = train(model,EPOCHS,decoder_criterion,full_model_optimizer,full_model_criterion,LEARNING_RATE,train_data_loader,valid_data_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z18bIFLHVecX"},"outputs":[],"source":["\n","def predict(model,iterator,mode):\n","\n","    predict_dict = next(iter(iterator))\n","    cover_image = predict_dict['cover_image']\n","    # print(cover_image)\n","    cover_image = cover_image.to(device)\n","    secret_image_1 = predict_dict['secret_image']\n","    secret_image_1 = secret_image_1.to(device)\n","\n","    model.eval()\n","\n","    if mode =='test':\n","      arnold = Arnold(6, 40, 33)\n","      coeffs, image = wtransform(cover_image[0].permute(1,2,0))\n","      soeffs, simage = wtransform(secret_image_1[0].permute(1,2,0))\n","      temp = [[],[],[]]\n","      coeff, cA, cH, cV, cD, soeff, sA, sH, sV, sD = [temp[:] for i in range(10)]\n","      \n","      # cA = arnold.applyTransformTo(cA)\n","      # cA = arnold.applyInverseTransformTo(cA)\n","      for ch in range(3):\n","        [(cA[ch], (cH[ch], cV[ch], cD[ch]))] = coeffs[ch]\n","        [(sA[ch], (sH[ch], sV[ch], sD[ch]))] = soeffs[ch]\n","        # coeff[ch]= [(cA, (cH, cV, cD))]\n","      \n","      cAA = torch.from_numpy(np.stack((cA[0], cA[1], cA[2]), axis=-1)).reshape(*cover_image.shape).to(device)\n","      cHH = torch.from_numpy(np.stack((cH[0], cH[1], cH[2]), axis=-1)).reshape(*cover_image.shape).to(device)\n","      cVV = torch.from_numpy(np.stack((cV[0], cV[1], cV[2]), axis=-1)).reshape(*cover_image.shape).to(device)\n","      cDD = torch.from_numpy(np.stack((cD[0], cD[1], cD[2]), axis=-1)).reshape(*cover_image.shape).to(device)\n","\n","      sAA = torch.from_numpy(np.stack((sA[0], sA[1], sA[2]), axis=-1)).reshape(*secret_image_1.shape).to(device)\n","      sHH = torch.from_numpy(np.stack((sH[0], sH[1], sH[2]), axis=-1)).reshape(*secret_image_1.shape).to(device)\n","      sVV = torch.from_numpy(np.stack((sV[0], sV[1], sV[2]), axis=-1)).reshape(*secret_image_1.shape).to(device)\n","      sDD = torch.from_numpy(np.stack((sD[0], sD[1], sD[2]), axis=-1)).reshape(*secret_image_1.shape).to(device)\n"," \n","      hA,rA= model(cAA,sAA,sAA,'full')\n","      hH,rH= model(cHH,sHH,sHH,'full')\n","      hV,rV= model(cVV,sVV,sVV,'full')\n","      hD,rD= model(cDD,sDD,sDD,'full')\n","\n","      rA = rA.to(torch.device('cpu')).detach().to(torch.long)[0].permute(1,2,0).numpy()\n","      rH = rH.to(torch.device('cpu')).detach().to(torch.long)[0].permute(1,2,0).numpy()\n","      rV = rV.to(torch.device('cpu')).detach().to(torch.long)[0].permute(1,2,0).numpy()\n","      rD = rD.to(torch.device('cpu')).detach().to(torch.long)[0].permute(1,2,0).numpy()\n","\n","      # rA= model(hH,hH,hH,'decoder')[0].permute(1,2,0).detach().numpy()\n","      # rH= model(hH,hH,hH,'decoder')[0].permute(1,2,0).detach().numpy()\n","      # rV= model(hV,hV,hV,'decoder')[0].permute(1,2,0).detach().numpy()\n","      # rD= model(hD,hD,hD,'decoder')[0].permute(1,2,0).detach().numpy()\n","      # rA = rA * 255\n","      # rA = rA.to(torch.device('cpu'))\n","      # rA = rA.detach().to(torch.long)\n","      # rA = rA[0].permute(1,2,0).numpy()\n","      # im = Image.fromarray(r.astype(np.uint8))\n","      # im.save(\"revealed.png\")\n","\n","      # print(hA)\n","      # print(\"latest\")\n","      # print(rA)\n","      for ch in range(3):\n","        soeff[ch]= [(rA[:,:,ch], (rH[:,:,ch], rV[:,:,ch], rD[:,:,ch]))]\n","     \n","    # test = Image.fromarray(np.uint8(cAA))\n","    # test.save(\"test.png\")\n","      reveal_image_1 = iwtransform(soeff,image)\n","      print(np.array(reveal_image_1))\n","      reveal_image_1.save(\"new.png\")\n","    if mode =='decoder':\n","      \n","      reveal_image_1= model(cover_image,cover_image,cover_image,mode)\n","      \n","      # dot_graph = torchviz.make_dot(model(cover_image,cover_image,cover_image,mode))\n","      # dot_graph.render(\"decoder.dot\")\n","    elif mode =='encoder':\n","      hidden_image= model(cover_image,secret_image_1,secret_image_1,mode)\n","      # dot_graph = torchviz.make_dot(model(cover_image,secret_image_1,secret_image_1,mode))\n","      # dot_graph.render(\"encoder.dot\")\n","    elif mode == \"full\":\n","      hidden_image,reveal_image_1= model(cover_image,secret_image_1,secret_image_1,mode)\n","      # dot_graph = torchviz.make_dot(model(cover_image,secret_image_1,secret_image_1,mode))\n","      # dot_graph.render(\"full.dot\")\n","      cover_image = cover_image * 255\n","      cover_image = cover_image.to(torch.device('cpu'))\n","      cover_image = cover_image.detach().to(torch.long)\n","      secret_image_1 = secret_image_1 * 255\n","      secret_image_1 = secret_image_1.to(torch.device('cpu'))\n","      secret_image_1 = secret_image_1.detach().to(torch.long)\n","    if mode =='encoder' or mode == 'full':\n","      hidden_image[hidden_image>1] = 1\n","      hidden_image = hidden_image * 255\n","      hidden_image = hidden_image.to(torch.device('cpu'))\n","      hidden_image = hidden_image.detach().to(torch.long)\n","      h = hidden_image[0].permute(1,2,0).numpy()\n","      im = Image.fromarray(h.astype(np.uint8))\n","      im.save(\"/content/drive/MyDrive/Colab Notebooks/stego_img.png\")\n","      if mode == 'encoder': return h\n","    if mode =='decoder' or mode == 'full':\n","      reveal_image_1[reveal_image_1>1] = 1\n","      reveal_image_1 = reveal_image_1 * 255\n","      reveal_image_1 = reveal_image_1.to(torch.device('cpu'))\n","      reveal_image_1 = reveal_image_1.detach().to(torch.long)\n","      r = reveal_image_1[0].permute(1,2,0).numpy()\n","      im = Image.fromarray(r.astype(np.uint8))\n","      im.save(\"/content/drive/MyDrive/Colab Notebooks/revealed.png\")\n","      if mode == 'decoder': return r\n","    return {\n","        'cover_image_grid':cover_image[0].permute(1,2,0).numpy(),\n","        'secret_image_1_grid':secret_image_1[0].permute(1,2,0).numpy(),\n","        'hidden_image_grid':h,\n","        'reveal_image_1_grid':r,\n","    }\n","grids = predict(model,test_data_loader,'full')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tN6-l4Py-gAh","executionInfo":{"status":"error","timestamp":1676000860439,"user_tz":-345,"elapsed":7147,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/","height":287},"outputId":"3e27dddc-5651-48ab-ae2e-72754b77bb98"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-6921186e7310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-33bbf63cc246>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, iterator, mode)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0msecret_image_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecret_image_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0msecret_image_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecret_image_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'encoder'\u001b[0m\u001b[0;34m|\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mhidden_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_image\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mhidden_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_image\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'str' and 'str'"]}],"source":["grids = predict(loaded_model,test_data_loader,'full')"]},{"cell_type":"code","source":["type(grids['cover_image_grid'])"],"metadata":{"id":"Fxp--7b5Ntrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7FdUR-Bqe2J"},"outputs":[],"source":["from torchvision.utils import save_image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0ioALQYWSwe"},"outputs":[],"source":["def plot(grids):\n","    plt.figure(figsize=(15,8))\n","    \n","    plt.subplot(241)\n","    plt.title('Cover Image')\n","    plt.imshow(grids['cover_image_grid'])\n","\n","    plt.subplot(242)\n","    plt.title('Secret Image')\n","    plt.imshow(grids['secret_image_1_grid'])\n","\n","    plt.subplot(245)\n","    plt.title('Hidden Image')\n","    plt.imshow(grids['hidden_image_grid'])\n","    \n","    plt.subplot(246)\n","    plt.title('Reveal Image')\n","    plt.imshow(grids['reveal_image_1_grid'])\n","\n","    plt.savefig('/content/drive/MyDrive/Colab Notebooks/Plot.png')\n","    plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzHpnf22U2vD"},"outputs":[],"source":["'''\n","data = next(iter(test_data_loader))\n","\n","cover_image = data['cover_image']\n","cover_image = cover_image.to(device)\n","\n","secret_image_1 = data['secret_image']\n","secret_image_1 = secret_image_1.to(device)\n","\n","model.eval()\n","    \n","hidden_image,reveal_image_1 = model(cover_image,secret_image_1,secret_image_1,'full')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MDSPFzuVIXM"},"outputs":[],"source":["#make_dot(hidden_image).render('hidden_image',format='png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3GlMMmYVNKx"},"outputs":[],"source":["#make_dot(reveal_image_1).render('reveal_image',format='png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woB2-YkIVSeo"},"outputs":[],"source":["'''\n","def plot_loss(loss_list):\n","    plt.plot(torch.tensor(loss_list, device = 'cpu'))\n","    plt.show()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ztDFJYfVTb5"},"outputs":[],"source":["#plot_loss(training_full_model_loss_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNk5S83VVWOf"},"outputs":[],"source":["#plot_loss(decoder_loss_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SIZqG9NWnOe"},"outputs":[],"source":["plot(grids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6hg_MrhWsI6"},"outputs":[],"source":["'''\n","for i in range(5):\n","    grids = predict(loaded_model,test_data_loader)\n","    plot(grids)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGtxtCQxyYM9"},"outputs":[],"source":["from matplotlib.image import imread\n","import matplotlib.pyplot as plt\n","import pywt\n","plt.rcParams['figure.figsize'] = [16, 16]\n","plt.rcParams.update({'font.size': 18})\n","\n","A = imread('/content/drive/MyDrive/Colab Notebooks/stego_img.png')\n","print(type(A))\n","B = np.mean(A, -1); # Convert RGB to grayscale\n","\n","n = 1\n","w = 'db1'\n","coeff = [[],[]]\n","arr = [[],[],[]]\n","coeff_slices = [[],[],[]]\n","for ch in range(3):\n","  coeffs = pywt.swt2(A[:,:,ch],wavelet=w,level=n)\n","  # data = np.zeros_like(A)\n","  # print(data.shape)\n","  # print(coeffs[0])\n","  # coeffs[1] = pywt.wavedec2(A[:,:,1],wavelet=w,level=n)\n","  # coeffs[2] = pywt.wavedec2(A[:,:,2],wavelet=w,level=n)\n","\n","  # normalize each coefficient array\n","  \n","  coeff[0] = coeffs[0][0] / np.abs(coeffs[0][0]).max()\n","  for detail_level in range(n):\n","      coeff[detail_level + 1] = [d/np.abs(d).max() for d in coeffs[0][detail_level + 1]]\n","\n","  arr[ch], coeff_slices[ch] = np.array(pywt.coeffs_to_array(coeff))\n","\n","final_img = np.dstack([arr[0]*255, arr[1]*255, arr[2]*255]).astype(np.uint8)\n","print(final_img.shape)\n","\n","# a = arr[coeff_slices[1]['dd']]\n","plt.imshow(final_img,cmap='gray',vmin=-0.25,vmax=0.75)\n","# plt.savefig('/content/drive/MyDrive/Colab Notebooks/Plotwave4.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQsCDAUSljqp"},"outputs":[],"source":["\n","r_np = imread('/content/drive/MyDrive/Colab Notebooks/Plotwave2.png')\n","g_np = imread('/content/drive/MyDrive/Colab Notebooks/Plotwave3.png')\n","b_np = imread('/content/drive/MyDrive/Colab Notebooks/Plotwave4.png')\n","\n","\n","# Add the channels to the final image\n","final_img = np.dstack([b_np[:,:,0], g_np[:,:,2], r_np[:,:,2]]).astype(np.uint8)\n","\n","print(final_img.shape)\n","plt.imshow(final_img, interpolation='nearest')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkPOtNqJ30NA"},"outputs":[],"source":["import numpy as np\n","import math, time, sys\n","from PIL import Image\n","class Arnold:\n","\n","    def __init__(self, a:int, b:int, rounds:int):\n","        # Parameters\n","        self.__a = a\n","        self.__b = b\n","        self.__rounds = rounds\n","\n","    def mapping(self, s:np.shape):\n","        x, y = np.meshgrid(range(s[0]), range(s[0]), indexing=\"ij\")\n","        xmap = (self.__a*self.__b*x + x + self.__a*y) % s[0]\n","        ymap = (self.__b*x + y) % s[0]\n","        return xmap, ymap\n","\n","    def inverseMapping(self, s:np.shape):\n","        x, y = np.meshgrid(range(s[0]), range(s[0]), indexing=\"ij\")\n","        xmap = (x - self.__a*y) % s[0]\n","        ymap = (-self.__b*x + self.__a*self.__b*y + y) % s[0]\n","        return xmap, ymap\n","\n","    def applyTransformTo(self, image:np.ndarray):\n","        xm, ym = self.mapping(image.shape)\n","        img = image\n","        for r in range(self.__rounds):\n","            img = img[xm, ym]\n","        return img\n","\n","    def applyInverseTransformTo(self, image:np.ndarray):\n","        xm, ym = self.inverseMapping(image.shape)\n","        img = image\n","        for r in range(self.__rounds):\n","          img = img[xm, ym]\n","        return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hczxr-re4FaQ"},"outputs":[],"source":["\n","image_path = \"/content/drive/MyDrive/Colab Notebooks/revealed.png\"\n","\n","# Arnold Transform Parameters\n","a = 6\n","b = 40\n","rounds = 33\n","\n","# Open the images\n","lena = np.array(Image.open(image_path))\n","\n","print(\" ~~~~  * PARAMETERS *  ~~~~ \")\n","arnold = Arnold(a, b, rounds)\n","print(\"\\ta:\\t\", a)\n","print(\"\\tb:\\t\", b)\n","print(\"\\trounds:\\t\", rounds)\n","\n","print(\"\\n ~~~~  *  RESULTS   *  ~~~~ \")\n","\n","start_time = time.time()\n","scrambled = arnold.applyTransformTo(lena)\n","exec_time = time.time() - start_time\n","print(\"Transform  execution time: %.6f \" % exec_time, \"sec\")\n","im = Image.fromarray(scrambled)\n","im.save(\"/content/drive/MyDrive/Colab Notebooks/scrambled111.tif\", format=\"TIFF\")\n","\n","start_time = time.time()\n","reconstructed = arnold.applyInverseTransformTo(scrambled)\n","exec_time = time.time() - start_time\n","print(\"Inverse T. execution time: %.6f \" % exec_time, \"sec\")\n","im = Image.fromarray(reconstructed)\n","im.save(\"/content/drive/MyDrive/Colab Notebooks/reconstructed1111.tif\", format=\"TIFF\")\n"]},{"cell_type":"code","source":["pip install opencv-python\n"],"metadata":{"id":"U3sNaALHAY14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fimg1 = cv2.imread(frames1[0])\n","fimg2 = cv2.imread(frames2[0])"],"metadata":{"id":"aRFlE5R8uAdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimension1=fimg1.shape[:2]\n","dimension1= tuple(reversed(dimension1))\n","dimension2=fimg2.shape[:2]\n","dimension2 = tuple(reversed(dimension2))"],"metadata":{"id":"gL9HwhDuuKU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimension1"],"metadata":{"id":"SlhBdx0V7Dx8","executionInfo":{"status":"ok","timestamp":1676000999424,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24bd782e-40e6-41ab-d05b-c32529d83020"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(600, 600)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[],"metadata":{"id":"2JWiLCcVudfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dimension1 = framelist1[0].shape[:2]\n","# dimension1 = tuple(reversed(dimension1))\n","# dimension2 = framelist2[0].shape[:2]\n","# dimension2 = tuple(reversed(dimension2))"],"metadata":{"id":"RFY2G1rl9wsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fdiff = len(dirs2)-len(dirs1)\n","print(\"frames of secret - frames of cover = \", fdiff)"],"metadata":{"id":"O87Rm86qRbl2","executionInfo":{"status":"ok","timestamp":1676001003236,"user_tz":-345,"elapsed":3,"user":{"displayName":"Manohar Dahal","userId":"04014046240579136312"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4407ccc-869c-4db2-8bc5-b20f24d84501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["frames of secret - frames of cover =  -1509\n"]}]},{"cell_type":"code","source":["import random\n","def increaseFrames(framelist,diff):\n","  dlist=[]\n","  for i in range(0,diff):\n","    temp = random.randint(0, len(dirs1)-1)\n","    if temp not in dlist:\n","      dlist.append(temp)\n","    else : \n","      while temp in dlist:\n","        temp = random.randint(0,len(dirs1)-1)\n","      dlist.append(temp)\n","  newlist=[]\n","  for i,frame in enumerate(framelist):\n","    if i not in dlist:\n","      newlist.append(frame)\n","    else:\n","      newlist.append(frame)\n","      newlist.append(frame)\n","  return newlist, dlist\n","      "],"metadata":{"id":"eoCo4v0bvJVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# if fdiff > 0:\n","#   modframelist1, modframes=increaseFrames(framelist1,fdiff)\n","#   framelist1 = modframelist1\n","\n","if fdiff > 0:\n","  modframelist1, modframes = increaseFrames(frames1,fdiff)\n","  frames1 = modframelist1\n","  del modframelist1\n","\n","\n"],"metadata":{"id":"7WQ9Tvl6_vG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stego_video=[]\n","# revealed_video = []\n","stego_dir= os.path.join(root_dir,'Stego-frames')\n","\n","if not os.path.isdir(stego_dir):\n","        os.mkdir(stego_dir)\n","for i in range(len(frames1)):\n","  if len(frames2) > i:\n","    cover = Image.open(frames1[i])\n","    secret = Image.open(frames2[i])\n","    # cover = Image.fromarray(cover)\n","    # secret = Image.fromarray(secret)\n","    vgrids = predict(loaded_model,test_data_loader,'full')\n","    stego_frame = vgrids['hidden_image_grid']\n","   \n","    im = Image.fromarray(stego_frame.astype(np.uint8))\n","    im.save(os.path.join(stego_dir,f'0000{i}.png'))\n","    \n","\n","\n","    # revealed_video.append(vgrids['reveal_image_1_grid'])\n","  else:\n","    Image.open(frames1[i]).save(os.path.join(stego_dir,f'0000{i}.png'))"],"metadata":{"id":"B9otqXGvw3RZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stego_vid = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/steggy_vid.mp4',cv2.VideoWriter_fourcc(*'MP4V'), fps1, dimension1)"],"metadata":{"id":"uHUpzH1RTjJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["revealed_vid = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/revealed_vid_.mp4',cv2.VideoWriter_fourcc(*'MP4V'), fps2, dimension1)"],"metadata":{"id":"jyGfwdoVTy3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dirsss=os.listdir(os.path.join(root_dir,'Stego-frames'))"],"metadata":{"id":"XSlx-FIx89Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frames3 = [os.path.join(root_dir,'Stego-frames',item) for item in dirsss]"],"metadata":{"id":"uSH65A_N9Vul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for frame in frames3:\n","    vidframe = cv2.imread(frame)\n","    stego_vid.write(vidframe)"],"metadata":{"id":"AhIE09ATT6j_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stego_vid.release()\n","# revealed_vid.release()"],"metadata":{"id":"GewewVjtUMUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["revealed_dir= os.path.join(root_dir,'revealed-frames')\n","\n","if not os.path.isdir(revealed_dir):\n","        os.mkdir(revealed_dir)\n","for i in range(len(frames3)):\n","  cover = Image.open(frames3[i])\n","  \n","  # cover = Image.fromarray(cover)\n","  # secret = Image.fromarray(secret)\n","  revealed_frame = predict(loaded_model,test_data_loader,'decoder')\n"," \n","  \n","  im = Image.fromarray(revealed_frame.astype(np.uint8))\n","  im.save(os.path.join(revealed_dir,f'0000{i}.png'))\n","  \n","\n","\n"],"metadata":{"id":"HH_CffKWAGRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dirs=os.listdir(os.path.join(root_dir,'revealed-frames'))\n"],"metadata":{"id":"CXXvT_wTDXxp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dirs[:5])"],"metadata":{"id":"54v1DesDDhEA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dirs)"],"metadata":{"id":"bMiY0tGrDlX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(frames4[:len(frames2)])"],"metadata":{"id":"q-QlDXBZRL7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frames4 = [os.path.join(root_dir,'revealed-frames',item) for item in dirs]"],"metadata":{"id":"DSst1hM7Doof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for frame in frames4[:len(frames2)]:\n","    vidframe = cv2.imread(frame)\n","    revealed_vid.write(vidframe)"],"metadata":{"id":"p_Y7DkKLDyoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["revealed_vid.release()"],"metadata":{"id":"gSSpadCSEYoX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1>Residue Codes</h1>"],"metadata":{"id":"_W1dhPOaHOBW"}},{"cell_type":"code","source":[],"metadata":{"id":"iJauiKFbHQqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Okcd6HINafB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cover = cv2.cvtColor(framelist1[0], cv2.COLOR_BGR2RGB)\n","secret = cv2.cvtColor(framelist2[0], cv2.COLOR_BGR2RGB)\n","cover = Image.fromarray(cover)\n","secret = Image.fromarray(secret)\n","\n","\n","cover = Image.open(cover)\n","secret = Image.open(secret)\n"],"metadata":{"id":"OfIK9Kx4BG-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgrids = predict(loaded_model,test_data_loader,'full')"],"metadata":{"id":"a6dTA1NyHPk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot(vgrids)"],"metadata":{"id":"23SOxfheIAgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","plt.imshow(cover)\n","plt.show()"],"metadata":{"id":"4Wy4IN0YIOrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow"],"metadata":{"id":"1UorRl_9Jrsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv2_imshow(modframelist1[0])"],"metadata":{"id":"d2t9o7fIJX01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stego_video=[]\n","# revealed_video = []\n","for i in range(len(framelist1)):\n","  if len(framelist2) > i:\n","    cover = cv2.cvtColor(framelist1[i], cv2.COLOR_BGR2RGB)\n","    secret = cv2.cvtColor(framelist2[i], cv2.COLOR_BGR2RGB)\n","    cover = Image.fromarray(cover)\n","    secret = Image.fromarray(secret)\n","    vgrids = predict(loaded_model,test_data_loader,'full')\n","    # stego_video.append(vgrids['hidden_image_grid'])\n","    # revealed_video.append(vgrids['reveal_image_1_grid'])\n","  else:\n","    # stego_video.append(framelist1[i])\n","    \n","\n"],"metadata":{"id":"s7uRwJacOQUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","def increaseFrames(framelist,diff):\n","  dlist=[]\n","  for i in range(0,diff):\n","    temp = random.randint(0,framecount1-1)\n","    if temp not in dlist:\n","      dlist.append(temp)\n","    else : \n","      while temp in dlist:\n","        temp = random.randint(0,framecount1-1)\n","      dlist.append(temp)\n","  newlist=[]\n","  for i,frame in enumerate(framelist):\n","    if i not in dlist:\n","      newlist.append(frame)\n","    else:\n","      newlist.append(frame)\n","      newlist.append(frame)\n","  return newlist, dlist\n","      "],"metadata":{"id":"FvT1oj3HHYNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimension1 = framelist1[0].shape[:2]\n","dimension1 = tuple(reversed(dimension1))\n","dimension2 = framelist2[0].shape[:2]\n","dimension2 = tuple(reversed(dimension2))"],"metadata":{"id":"FT2QbE-SHyhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def format_timedelta(td):\n","#     \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n","#     omitting microseconds and retaining milliseconds\"\"\"\n","#     result = str(td)\n","#     try:\n","#         result, ms = result.split(\".\")\n","#     except ValueError:\n","#         return (result + \".00\").replace(\":\", \"-\")\n","#     ms = int(ms)\n","#     ms = round(ms / 1e4)\n","#     return f\"{result}.{ms:02}\".replace(\":\", \"-\")"],"metadata":{"id":"kMey6C9yG7sy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def get_saving_frames_durations(cap, saving_fps):\n","#     \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n","#     s = []\n","#     # get the clip duration by dividing number of frames by the number of frames per second\n","#     clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n","#     # use np.arange() to make floating-point steps\n","#     for i in np.arange(0, clip_duration, 1 / saving_fps):\n","#         s.append(i)\n","#     return s"],"metadata":{"id":"4_NjcJVaICBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def toFrames(video_file):\n","#     # filename, _ = os.path.splitext(video_file)\n","#     # filename += \"-opencv\"\n","#     # # make a folder by the name of the video file\n","#     # if not os.path.isdir(filename):\n","#     #     os.mkdir(filename)\n","#     # # read the video file    \n","#     cap = cv2.VideoCapture(video_file)\n","#     # get the FPS of the video\n","#     fps = cap.get(cv2.CAP_PROP_FPS)\n","#     # # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n","#     # saving_frames_per_second = fps\n","#     # # get the list of duration spots to save\n","#     # saving_frames_durations = get_saving_frames_durations(cap, saving_frames_per_second)\n","#     # start the loop\n","#     count = 0\n","    \n","#     while True:\n","#         is_read, frame = cap.read()\n","#         if not is_read:\n","#             # break out of the loop if there are no frames to read\n","#             break\n","            \n","#         # # get the duration by dividing the frame count by the FPS\n","#         # frame_duration = count / fps\n","#         # try:\n","#         # #     # get the earliest duration to save\n","#         #     closest_duration = saving_frames_durations[0]\n","#         # except IndexError:\n","#         # #     # the list is empty, all duration frames were saved\n","#         #     break\n","#         # if frame_duration >= closest_duration:\n","#         # #     if closest duration is less than or equals the frame duration, \n","#         # #     then save the frame\n","          \n","#           # frame_duration_formatted = format_timedelta(timedelta(seconds=frame_duration))\n","#         print(frame)\n","#             # drop the duration spot from the list, since this duration spot is already saved\n","#           # try:\n","#           #     saving_frames_durations.pop(0)\n","#           # except IndexError:\n","#           #     pass\n","#         # increment the frame count\n","#         # framelist.append(frame)\n","#         count += 1\n","#     print('total number of frames =', count)\n","#     # return framelist\n"],"metadata":{"id":"NwQG0D6qIQz7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1hqizSnUF4nCX1-UmiIQfssD5Y6f3P-Xv","timestamp":1672912159339}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}